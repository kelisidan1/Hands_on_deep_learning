{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 大纲\n",
    "这篇笔记主要涉及到四个方面\n",
    "1. 模型构造\n",
    "2. 参数管理\n",
    "3. 自定义层\n",
    "4. 读写文件"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 层和块\n",
    "首先回顾一下 多层感知机 使用Sequential方法构造的网络\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.2930, -0.1645,  0.0174, -0.3973, -0.1224, -0.1972,  0.0418, -0.0820,\n         -0.1930,  0.0182],\n        [-0.1484, -0.1867, -0.1066, -0.2786, -0.0379, -0.1226, -0.0315, -0.0231,\n         -0.1069,  0.0424]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "net = nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10)) # 将 输入层 - 隐藏层 - 输出层 通过Sequential 链接在一起。\n",
    "X = torch.rand(2,20) # 生成一个矩阵 2 * 20\n",
    "net(X) # 前向计算"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pytorch --- Module"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.1185,  0.0124,  0.0934,  0.1295, -0.1545, -0.2100, -0.2180, -0.0600,\n          0.0885,  0.0562],\n        [ 0.1085,  0.0238,  0.1909,  0.0130, -0.1437, -0.2066, -0.1825, -0.0702,\n          0.0052,  0.1233]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP() # 实例化多层感知机的层\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# nn.Sequential()是怎么实现的？"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.2693, -0.1063, -0.0858,  0.0596,  0.1992,  0.2189, -0.2238,  0.2035,\n          0.0782, -0.1133],\n        [ 0.3423, -0.1310, -0.0784,  0.0876,  0.1457,  0.1219, -0.2661,  0.2284,\n          0.1043,  0.0037]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            self._modules[block] = block # _modules是一个特殊的容器，当参数传入到这个容器的时候，pytorch会自动认为这个东西是神经网络当中的一个层。\n",
    "\n",
    "    def forward(self,X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4787, 0.4555, 0.8340, 0.5646, 0.0984, 0.8725, 0.8583, 0.2085, 0.4287,\n",
      "         0.2374, 0.9525, 0.8228, 0.1279, 0.2299, 0.8118, 0.3331, 0.0433, 0.9108,\n",
      "         0.9465, 0.4644],\n",
      "        [0.6777, 0.2984, 0.4431, 0.5390, 0.1228, 0.7911, 0.5630, 0.9455, 0.2324,\n",
      "         0.1269, 0.5150, 0.0213, 0.2741, 0.7813, 0.4960, 0.7490, 0.6989, 0.4944,\n",
      "         0.3671, 0.6595],\n",
      "        [0.3795, 0.6453, 0.9233, 0.4395, 0.7294, 0.9995, 0.1167, 0.4569, 0.2691,\n",
      "         0.4039, 0.2984, 0.5077, 0.6481, 0.2042, 0.3834, 0.0487, 0.1255, 0.8748,\n",
      "         0.4785, 0.4881],\n",
      "        [0.2682, 0.4521, 0.1652, 0.3652, 0.0805, 0.2901, 0.2636, 0.1779, 0.3420,\n",
      "         0.2244, 0.7577, 0.0812, 0.3160, 0.0157, 0.6461, 0.7249, 0.9319, 0.2615,\n",
      "         0.3178, 0.0169],\n",
      "        [0.0492, 0.3975, 0.1418, 0.7386, 0.3162, 0.8662, 0.5520, 0.9778, 0.2952,\n",
      "         0.4046, 0.4751, 0.8925, 0.2058, 0.0923, 0.5856, 0.6992, 0.3536, 0.2148,\n",
      "         0.5359, 0.7697],\n",
      "        [0.0020, 0.4321, 0.7644, 0.0665, 0.7197, 0.5561, 0.2844, 0.8456, 0.6355,\n",
      "         0.4745, 0.1818, 0.7565, 0.0793, 0.4074, 0.2546, 0.9586, 0.1479, 0.2633,\n",
      "         0.8360, 0.6231],\n",
      "        [0.9010, 0.8147, 0.8937, 0.3841, 0.9070, 0.2946, 0.9890, 0.1020, 0.5797,\n",
      "         0.7829, 0.6679, 0.3699, 0.4891, 0.3080, 0.7369, 0.9274, 0.3566, 0.2092,\n",
      "         0.5112, 0.0927],\n",
      "        [0.0942, 0.5688, 0.2108, 0.0081, 0.5738, 0.8698, 0.2115, 0.6336, 0.0796,\n",
      "         0.6723, 0.0169, 0.5338, 0.3488, 0.9443, 0.8918, 0.6059, 0.0969, 0.7002,\n",
      "         0.0607, 0.0764],\n",
      "        [0.1936, 0.9143, 0.8790, 0.9212, 0.2600, 0.8710, 0.3695, 0.9959, 0.5656,\n",
      "         0.8869, 0.0360, 0.5755, 0.2731, 0.2300, 0.4762, 0.2075, 0.3614, 0.1331,\n",
      "         0.1916, 0.1024],\n",
      "        [0.0032, 0.5004, 0.5952, 0.5851, 0.5086, 0.8682, 0.5438, 0.2288, 0.4436,\n",
      "         0.1988, 0.2162, 0.5343, 0.2960, 0.6185, 0.0120, 0.8376, 0.4146, 0.2880,\n",
      "         0.9720, 0.4136],\n",
      "        [0.6713, 0.2322, 0.8172, 0.1425, 0.1217, 0.6349, 0.7295, 0.3670, 0.2297,\n",
      "         0.5278, 0.7415, 0.4434, 0.8771, 0.5800, 0.8230, 0.5011, 0.5267, 0.8099,\n",
      "         0.6269, 0.6611],\n",
      "        [0.0054, 0.2135, 0.3931, 0.2569, 0.3234, 0.9025, 0.1512, 0.4007, 0.4592,\n",
      "         0.6450, 0.0741, 0.6430, 0.2135, 0.7278, 0.8364, 0.6537, 0.1897, 0.7829,\n",
      "         0.7397, 0.4377],\n",
      "        [0.0388, 0.3384, 0.7671, 0.2342, 0.6960, 0.8795, 0.6555, 0.8921, 0.1993,\n",
      "         0.6058, 0.7528, 0.2741, 0.0064, 0.2481, 0.1523, 0.0920, 0.2572, 0.1688,\n",
      "         0.9427, 0.0667],\n",
      "        [0.8847, 0.3320, 0.9200, 0.6954, 0.7139, 0.7695, 0.5160, 0.7188, 0.3516,\n",
      "         0.4027, 0.6683, 0.0141, 0.9354, 0.5646, 0.5564, 0.2583, 0.9547, 0.8512,\n",
      "         0.9493, 0.0083],\n",
      "        [0.8531, 0.8223, 0.0708, 0.8421, 0.0200, 0.1873, 0.9505, 0.4282, 0.4938,\n",
      "         0.6981, 0.9954, 0.0863, 0.8742, 0.1484, 0.1793, 0.5381, 0.7641, 0.1155,\n",
      "         0.6531, 0.9931],\n",
      "        [0.0155, 0.6674, 0.4679, 0.7299, 0.5349, 0.8124, 0.0520, 0.6923, 0.0870,\n",
      "         0.7511, 0.3567, 0.5363, 0.5901, 0.5153, 0.8261, 0.2146, 0.4394, 0.7596,\n",
      "         0.6008, 0.4382],\n",
      "        [0.4882, 0.0861, 0.7957, 0.1519, 0.8724, 0.6508, 0.9894, 0.8329, 0.8724,\n",
      "         0.3253, 0.6187, 0.8568, 0.3309, 0.5415, 0.4539, 0.9057, 0.8797, 0.8115,\n",
      "         0.2387, 0.8867],\n",
      "        [0.6769, 0.2760, 0.8739, 0.1517, 0.7133, 0.2408, 0.8320, 0.4166, 0.4472,\n",
      "         0.2580, 0.3077, 0.2076, 0.1998, 0.6300, 0.7944, 0.4517, 0.7736, 0.2247,\n",
      "         0.8825, 0.1231],\n",
      "        [0.3368, 0.0549, 0.6005, 0.6105, 0.0917, 0.8002, 0.9683, 0.4952, 0.7174,\n",
      "         0.6980, 0.4867, 0.4437, 0.8340, 0.9682, 0.5164, 0.0213, 0.4951, 0.6551,\n",
      "         0.7403, 0.6680],\n",
      "        [0.6550, 0.3010, 0.8411, 0.3706, 0.5665, 0.7888, 0.4265, 0.0561, 0.5646,\n",
      "         0.5091, 0.9167, 0.2267, 0.0076, 0.8232, 0.5706, 0.8374, 0.2646, 0.6062,\n",
      "         0.8546, 0.9692]])\n",
      "tensor([[-0.3104, -0.5091, -0.1208, -0.1678, -0.5821, -0.2213, -0.1021, -0.4598,\n",
      "         -0.0404, -0.5439, -0.2567, -0.0573,  0.0667,  0.2089,  0.4255, -0.7989,\n",
      "         -0.0811,  0.2751, -0.7924,  0.7203],\n",
      "        [-0.2693, -0.3674, -0.2510,  0.1013, -0.3367, -0.2503, -0.0689, -0.4506,\n",
      "         -0.2550, -0.2588, -0.1990, -0.0295, -0.0644,  0.1715,  0.3314, -0.4762,\n",
      "         -0.0194,  0.2888, -0.6462,  0.9182]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(-0.1190, grad_fn=<SumBackward0>)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20,20),requires_grad=False)\n",
    "        self.linear = nn.Linear(20,20)\n",
    "\n",
    "    def forward(self,X):\n",
    "        X = self.linear(X) #\n",
    "        X = F.relu(torch.mm(X,self.rand_weight)+1)\n",
    "        X = self.linear(X)\n",
    "        while X.abs().sum() > 1:\n",
    "            X/=2\n",
    "        return X.sum()\n",
    "\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 混合搭配各种组合块的方法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(20,64),nn.ReLU(),\n",
    "            nn.Linear(64,32),nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Linear(32,16)\n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(\n",
    "    NestMLP(),nn.Linear(16,20),FixedHiddenMLP()\n",
    ")\n",
    "chimera(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 参数管理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.3232],\n        [0.2353]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4,8),nn.ReLU(),\n",
    "    nn.Linear(8,1)\n",
    ")\n",
    "X = torch.rand(size=(2,4))\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 参数访问 - 将每一层的参数拿出来"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.0285, -0.0015,  0.0136,  0.1205],\n",
      "        [-0.4237, -0.1148, -0.0061, -0.4701],\n",
      "        [ 0.3143, -0.3204,  0.3919,  0.2768],\n",
      "        [-0.4206,  0.4978, -0.0646,  0.0909],\n",
      "        [-0.1121, -0.2425,  0.0699,  0.4536],\n",
      "        [-0.0639,  0.2748,  0.0149,  0.3290],\n",
      "        [ 0.1113,  0.1409, -0.1603,  0.2750],\n",
      "        [-0.4267, -0.2436,  0.2722,  0.2949]])), ('bias', tensor([ 0.3514, -0.0768,  0.0453, -0.1184, -0.3697,  0.2860, -0.2149, -0.0447]))])\n",
      "ReLU()\n",
      "OrderedDict([('weight', tensor([[ 0.2537, -0.1772, -0.1563,  0.1596, -0.1651,  0.1239,  0.2718, -0.2204]])), ('bias', tensor([0.2124]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[0].state_dict())\n",
    "print(net[1])\n",
    "print(net[2].state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.2124], requires_grad=True)\n",
      "tensor([0.2124])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias)) # Parameter是可以优化的参数\n",
    "print(net[2].bias) #他是一个tensor\n",
    "print(net[2].bias.data) # 获取到他的值 就是 y = kx + b 当中 b的值"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里还没有进行反向计算呢，所以它是None\n",
    "net[2].weight.grad == None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "# 一次性访问所有参数\n",
    "print(*[(name,param.shape) for name,param in net[0].named_parameters()])\n",
    "print(*[(name,param.shape) for name,param in net.named_parameters()])\n",
    "# 解释一下第二个东西，它只写了 net[0]的东西 和 net[2]的东西，是因为net[1]是relu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.2124])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data # 拿到第二层的偏移"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.1823],\n        [0.1823]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4,8),nn.ReLU(),\n",
    "        nn.Linear(8,4),nn.ReLU()\n",
    "    )\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block{i}',block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(),nn.Linear(4,1))\n",
    "rgnet(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0064,  0.0151,  0.0088, -0.0151]) tensor(0.)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([0.0035, 0.0097, 0.0018, 0.0049]), tensor(0.))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_normal) # 对net当中所有的layer 都进行init\n",
    "net[0].weight.data[0],net[0].bias.data[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([1., 1., 1., 1.]), tensor(0.))"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight,1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0],net[0].bias.data[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5329,  0.0982, -0.2457,  0.6814])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "# 可以对不同的层使用不同的初始化函数\n",
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight,42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# 参数绑定\n",
    "shared = nn.Linear(8,8)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4,8),nn.ReLU(),\n",
    "    shared,nn.ReLU(),\n",
    "    shared,nn.ReLU(),\n",
    "    nn.Linear(8,1)\n",
    ") # 共享layer参数\n",
    "net(X)\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0,0] = 100\n",
    "print(net[2].weight.data == net[4].weight.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 自定义层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0., 0., 0., 1., 2.]), tensor(0.0962))"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,X):\n",
    "        return X - X.mean()\n",
    "\n",
    "net = nn.Sequential(\n",
    "    CenteredLayer(),nn.ReLU()\n",
    ")\n",
    "net(torch.FloatTensor([1,2,3,4,5])) ,net(torch.rand(4,8)).mean() #理解网络 最简单的方式\n",
    "# mean()函数对matrix也有效果"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "(Parameter containing:\n tensor([[-0.8371, -1.5490,  0.2123],\n         [-1.1814, -0.7740, -0.3956],\n         [ 1.0152, -1.1315,  0.7895],\n         [ 0.9769,  0.7218,  2.0614],\n         [ 0.1837,  0.2139, -2.5674]], requires_grad=True),\n tensor([[0.0000, 0.0000, 1.8493]]))"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self,in_units,units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units,units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "\n",
    "    def forward(self,X):\n",
    "        linear = torch.matmul(X,self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)\n",
    "\n",
    "dense = MyLinear(5,3)\n",
    "dense.weight,dense(torch.randn(1,5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.9562, -0.5663, -0.0109],\n        [ 1.9909,  1.9502, -0.8271],\n        [ 0.0052,  0.0498, -0.3538],\n        [ 0.1803,  0.7834, -1.2245],\n        [ 0.4991, -0.5466,  0.3520]])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(5,3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[13.1029],\n        [ 2.7483]])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    MyLinear(64,8),MyLinear(8,1)\n",
    ")\n",
    "net(torch.rand(2,64))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 读写文件"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 1, 2, 3])"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x,'x-file.pt')\n",
    "\n",
    "x2 = torch.load(\"x-file.pt\")\n",
    "x2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x,y],'x-files')\n",
    "x2,y2 = torch.load('x-files')\n",
    "(x2,y2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x':x,'y':y}\n",
    "torch.save(mydict,'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 保存模型参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.output = nn.Linear(256,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2,20))\n",
    "Y = net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),'mlp.params')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "MLP(\n  (hidden): Linear(in_features=20, out_features=256, bias=True)\n  (output): Linear(in_features=256, out_features=10, bias=True)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们上面已经存储了网络的参数，接下来，我们可以先创建一个网络，然后讲这些个参数直接带入到网络当中。\n",
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load(\"mlp.params\"))\n",
    "clone.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[True, True, True, True, True, True, True, True, True, True],\n        [True, True, True, True, True, True, True, True, True, True]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
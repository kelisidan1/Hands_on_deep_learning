{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([5.]), tensor([6.]), tensor([1.5000]), tensor([9.]))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([3.0])\n",
    "y = torch.tensor([2.0])\n",
    "\n",
    "x+y,x*y,x/y,x**y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(3.)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4,dtype=torch.float32)\n",
    "x[3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0,  1,  2,  3,  4],\n         [ 5,  6,  7,  8,  9],\n         [10, 11, 12, 13, 14],\n         [15, 16, 17, 18, 19]]),\n tensor([[ 0,  5, 10, 15],\n         [ 1,  6, 11, 16],\n         [ 2,  7, 12, 17],\n         [ 3,  8, 13, 18],\n         [ 4,  9, 14, 19]]))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.arange(20).reshape((4,5))\n",
    "B,B.T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 5, 4]), tensor(780., dtype=torch.float64))"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20*2,dtype=float).reshape(2,5,4)\n",
    "A.shape,A.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 如果想对一个维度进行求和，应该怎么做呢？\n",
    "> 就比如，几个图片叠加到一起，我想合并成一张图片，最简单的想法就是对应像素点处的像素值进行相加\n",
    "> 不懂就看下边的例子，一看就明白"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.],\n          [16., 17., 18., 19.]],\n \n         [[20., 21., 22., 23.],\n          [24., 25., 26., 27.],\n          [28., 29., 30., 31.],\n          [32., 33., 34., 35.],\n          [36., 37., 38., 39.]]], dtype=torch.float64),\n tensor([[20., 22., 24., 26.],\n         [28., 30., 32., 34.],\n         [36., 38., 40., 42.],\n         [44., 46., 48., 50.],\n         [52., 54., 56., 58.]], dtype=torch.float64),\n torch.Size([5, 4]),\n tensor(19.5000, dtype=torch.float64))"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A,A_sum_axis0,A_sum_axis0.shape,A.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "axis = 0  -> 层\n",
    "axis = 1  -> 行\n",
    "axis = 2  -> 列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[10., 11., 12., 13.],\n         [14., 15., 16., 17.],\n         [18., 19., 20., 21.],\n         [22., 23., 24., 25.],\n         [26., 27., 28., 29.]], dtype=torch.float64),\n tensor([[ 8.,  9., 10., 11.],\n         [28., 29., 30., 31.]], dtype=torch.float64),\n tensor([[ 1.5000,  5.5000,  9.5000, 13.5000, 17.5000],\n         [21.5000, 25.5000, 29.5000, 33.5000, 37.5000]], dtype=torch.float64))"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0),A.mean(axis=1),A.mean(axis=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "累加求和："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.],\n         [16., 17., 18., 19.]]),\n tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  6.,  8., 10.],\n         [12., 15., 18., 21.],\n         [24., 28., 32., 36.],\n         [40., 45., 50., 55.]]))"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20,dtype=torch.float32).reshape(5,4)\n",
    "A,A.cumsum(axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "向量的点积"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(4,dtype=torch.float32)\n",
    "x,y,torch.dot(x,y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "矩阵的点积"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.],\n         [16., 17., 18., 19.]]),\n tensor([0., 1., 2., 3.]),\n torch.Size([5, 4]),\n torch.Size([4]),\n tensor([ 14.,  38.,  62.,  86., 110.]))"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A,x,A.shape,x.shape,torch.mv(A,x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "范数的实现："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(5.)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0,-4.0])\n",
    "torch.norm(u) # 向量二范数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(7.)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum() #向量一范数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(6.)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones((4,9))) # 矩阵F范数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}